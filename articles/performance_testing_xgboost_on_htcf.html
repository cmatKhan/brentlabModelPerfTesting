<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="brentlabModelPerfTesting">
<title>performance_testing_xgboost_on_htcf • brentlabModelPerfTesting</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="performance_testing_xgboost_on_htcf">
<meta property="og:description" content="brentlabModelPerfTesting">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">brentlabModelPerfTesting</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/Usage.html">Usage</a>
    <a class="dropdown-item" href="../articles/Using_Package_Data.html">Using_Package_Data</a>
    <a class="dropdown-item" href="../articles/fully_featured_R_package_development.html">fully_featured_R_package_development</a>
    <a class="dropdown-item" href="../articles/performance_testing_xgboost_on_htcf.html">performance_testing_xgboost_on_htcf</a>
  </div>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav"></ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>performance_testing_xgboost_on_htcf</h1>
            
      
      
      <div class="d-none name"><code>performance_testing_xgboost_on_htcf.Rmd</code></div>
    </div>

    
    
<style type="text/css">
.main-container {
  max-width: 2500px;
  margin-left: auto;
  margin-right: auto;
}
</style>
<div class="section level2">
<h2 id="application">Application<a class="anchor" aria-label="anchor" href="#application"></a>
</h2>
<p>We wish to do the following:</p>
<ol style="list-style-type: decimal">
<li>extract data from a <em>genomic feature</em> by <em>sample</em>
expression matrix</li>
<li>extract data from a <em>variant</em> by <em>variant label</em> VCF
style file</li>
<li>convert this data into a modelling format</li>
<li>clean and prepare this data – ie, deal with NAs, drop zero variance
features from the model matrix</li>
<li>train a model</li>
<li>tune hyperparameters via grid search and n-fold cross
validation</li>
<li>Output the data for downstream processing</li>
</ol>
<p>Current application requires this to be done over genes, which means
that this process must be performed on 10,000 to 20,000 genes, up to 10x
each for hyperparameter tuning/cross validation.</p>
</div>
<div class="section level2">
<h2 id="goal">Goal<a class="anchor" aria-label="anchor" href="#goal"></a>
</h2>
<p>A flexible, repeatable, reproducible environment in which to explore
the resource usage of various models in order to use efficiently the
computing resources that we have available.</p>
</div>
<div class="section level2">
<h2 id="questions-and-answers">Questions and Answers<a class="anchor" aria-label="anchor" href="#questions-and-answers"></a>
</h2>
<p><strong>Which parameters most affect runtime and resource
usage</strong></p>
<blockquote>
<p>Number of features of course most impact memory. However, this can be
significantly reduced by using sparse matrix data representations when
the underlying software can handle it. Average memory usage for the
large model was around 4GB - 6GB. I opted for a 10GB SBATCH request.</p>
</blockquote>
<blockquote>
<p>The number of rounds had the greatest effect on runtime. For a given
number of rounds, the number of features has the greatest impact.
max_bin and max_depth had little to no impact.</p>
</blockquote>
<blockquote>
<p>With 7 CPUs, the runtime for a single model is about 3.6 minutes on
my local. Interesting, there was more variability on the cluster.</p>
</blockquote>
<blockquote>
<p>GPU execution on the cluster, on the same model and settings, took
about 22 seconds. The GPU is therefore ~ 10x faster on a per model
basis</p>
</blockquote>
<p><strong>What is the scheduling rate on the CPUs and GPUs on
HTCF?</strong></p>
<ul>
<li>CPUs</li>
</ul>
<blockquote>
<p>With 81k features, 1k rounds and max_depth and max_bin both set to
1000, the scheduling rate on the CPU is about 30 jobs per minute.</p>
</blockquote>
<blockquote>
<p>With a 10k feature model, 10k rounds, max_depth = 2 and the XGBoost
default max_bin (256), the scheduling rate is 84 jobs per minute.
<strong><em>This means that to do 10 fold cross validation on 20,000
genes, with no (statistical theory based) speed-ups from sharing data
across folds (this is at least available in <code>caret</code>, possibly
<code>scikit-learn</code>), the runtime would be about 1.6
days</em></strong></p>
</blockquote>
<ul>
<li>GPUs</li>
</ul>
<blockquote>
<p>The scheduling rate with 81k features and 1k rounds was 8.13 per
minute when scheduling a typical array job (1 model per resource
request). It occurred to me that since each model runs so much faster,
it might be possible to speed up the runtime by ‘batching’ the jobs so
that some number of models run sequentially per resource request. For
instance, to run 1000 models, you would submit an array job of 100
tasks, and in each one of those 100 tasks, 10 models would run
sequentially. However, this didn’t improve the <em>per model</em>
execution rate – the result of 10 sequential tasks per submission was ~6
jobs per minute. This probably warrants further investigation,
<strong>however</strong> there are a number options to speed up the CPU
execution, already comparatively fast, which will be far easier to
implement and result in easier to distribute software. Therefore, using
the GPUs on the scale of HTCF to do genome scale work is very unlikely
to have worthwhile returns in terms of runtime optimization.</p>
</blockquote>
</div>
<div class="section level2">
<h2 id="conclusion">Conclusion<a class="anchor" aria-label="anchor" href="#conclusion"></a>
</h2>
<p>For ‘full’ genomic scale tasks, due to the high level of concurrency
that is afforded by so many CPUs on the general compute partition, it
will generally be faster to use the CPUs. However, it is possible that
by chunking jobs, so that multiple models run sequentially per resource
request, that the runtime could be reduced by utilizing the GPUs for the
modelling task.</p>
<p>For single to a small number of model tasks – eg, doing EDA on some
genes with different expression characteristics, or exploring some of
the outlier-y models from a full scale genome run – using the GPU will
be ~ 10x faster than using the CPU. That said, the cluster isn’t your
personal computer. Submit your jobs via sbatch. Don’t just sit on
resources.</p>
</div>
<div class="section level2">
<h2 id="details">Details<a class="anchor" aria-label="anchor" href="#details"></a>
</h2>
<div class="section level3">
<h3 id="run-time-and-memory-on-cpus-and-gpus-single-gene-model">Run time and memory on CPUs and GPU(s) – single gene model<a class="anchor" aria-label="anchor" href="#run-time-and-memory-on-cpus-and-gpus-single-gene-model"></a>
</h3>
<p>parameters of interest which affect run time and memory usage:</p>
<ol style="list-style-type: decimal">
<li>the number of features</li>
<li>the number of trees</li>
<li>the number of rounds</li>
</ol>
<p>In each case, we want to vary either the number of CPUs, or run the
models on the GPU(s).</p>
<div class="section level4">
<h4 id="results">Results<a class="anchor" aria-label="anchor" href="#results"></a>
</h4>
<div class="section level5">
<h5 id="overview">Overview<a class="anchor" aria-label="anchor" href="#overview"></a>
</h5>
<p><img src="performance_testing_xgboost_on_htcf_files/figure-html/unnamed-chunk-2-1.png" width="2880"></p>
</div>
<div class="section level5">
<h5 id="the-largest-model">The Largest Model<a class="anchor" aria-label="anchor" href="#the-largest-model"></a>
</h5>
<p><img src="performance_testing_xgboost_on_htcf_files/figure-html/unnamed-chunk-3-1.png" width="2880"></p>
</div>
</div>
</div>
<div class="section level3">
<h3 id="submission-rate">Submission rate<a class="anchor" aria-label="anchor" href="#submission-rate"></a>
</h3>
<p>The optimal CPU request seems to be between 3 and 7 CPUs. More than 7
offers significantly decreased improvement. Closer to 7 is likely best,
but smaller resource requests typically increase job submission rate,
which impacts concurrency – as we’ve seen, this has a huge impact on
runtime. To test the submission rate on the cluster, I containerized the
virtual environment (including this package), wrote a cmd line interface
which can be accessed from the installed package like so:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/system.file.html" class="external-link">system.file</a></span><span class="op">(</span><span class="st">'xgboost_perf_testing.R'</span>, package <span class="op">=</span> <span class="st">'brentlabModelPerfTesting'</span><span class="op">)</span></span></code></pre></div>
<p>Or in the <code>inst</code> directory in the github repo.</p>
<p>This is submitted like so for CPU execution</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --mem=10G</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --cpus-per-task=8</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --time=10</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --job-name=rate_testing</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --output=rate_testing.out</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="bu">eval</span> <span class="va">$(</span><span class="ex">spack</span> load <span class="at">--sh</span> singularityce@3.8.0<span class="va">)</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="va">singularity_image</span><span class="op">=</span><span class="va">$1</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="va">run_script</span><span class="op">=</span>/scratch/mblab/chasem/xgboost_testing/brentlabModelPerfTesting/inst/xgboost_pref_testing.R</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="va">rounds</span><span class="op">=</span>10000</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="va">max_bin</span><span class="op">=</span>256</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="va">max_depth</span><span class="op">=</span>2</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="ex">singularity</span> exec <span class="dt">\</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>  <span class="at">-B</span> /scratch/mblab <span class="dt">\</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>  <span class="at">-B</span> <span class="st">"</span><span class="va">$PWD</span><span class="st">"</span> <span class="dt">\</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>  <span class="va">$singularity_image</span> <span class="dt">\</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>  /bin/bash <span class="at">-c</span> <span class="st">"cd </span><span class="va">$PWD</span><span class="st">; </span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="st"> </span><span class="va">$run_script</span><span class="st"> --cpu 7 --rounds </span><span class="va">$rounds</span><span class="st"> --max_bin </span><span class="va">$max_bin</span><span class="st"> --max_depth </span><span class="va">$max_depth</span><span class="st">"</span></span></code></pre></div>
<p>with the submission cmd</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="ex">sbatch</span> <span class="at">--array</span><span class="op">=</span>1-1000 submit.sh ../software/brentlabxgboost_latest.sif</span></code></pre></div>
<p>To test this on the <code>GPU</code>, only three items need be
changed on the SBATCH submission</p>
<ul>
<li>the resource requests</li>
<li>the singularity flag <code>--nv</code> must be set</li>
<li>set the <code>--gpu</code> flag in the cmd line script</li>
</ul>
<div class="sourceCode" id="cb4"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH -p gpu</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --gpus=1</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --mem=10G</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --time=10</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --job-name=rate_testing</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">#SBATCH --output=rate_testing.out</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co"># ... same as above</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># note that new --nv flag!</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="ex">singularity</span> exec <span class="dt">\</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">--nv</span>  <span class="dt">\</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">-B</span> /scratch/mblab <span class="dt">\</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">-B</span> <span class="st">"</span><span class="va">$PWD</span><span class="st">"</span> <span class="dt">\</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a> <span class="va">$run_script</span> <span class="at">--gpu</span> <span class="at">--rounds</span> <span class="va">$rounds</span> <span class="at">--max_bin</span> <span class="va">$max_bin</span> <span class="at">--max_depth</span> <span class="va">$max_depth</span><span class="st">"</span></span></code></pre></div>
<p>It should be clear how to modify the sbatch submission to do a grid
search over, in this case, the parameters which affect run time and
memory usage. Just create a data.frame (<code>expand.grid</code> in R.
<a href="https://stackoverflow.com/questions/12130883/r-expand-grid-function-in-python" class="external-link">There
is a long discussion here on how to do the same in python</a>. I’d check
numpy, too). However you do this, write it as a <code>tsv</code>:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cpu_testing_grid</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expand.grid.html" class="external-link">expand.grid</a></span><span class="op">(</span></span>
<span>  features <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1000</span>,<span class="fl">10000</span>,<span class="fu"><a href="https://rdrr.io/r/base/nrow.html" class="external-link">ncol</a></span><span class="op">(</span><span class="va">gene_data</span><span class="op">)</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span>, <span class="co"># minus 1 since first col is response</span></span>
<span>  rounds <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">10</span>,<span class="fl">100</span>,<span class="fl">1000</span><span class="op">)</span>,</span>
<span>  max_depth  <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">100</span>,<span class="fl">500</span>,<span class="fl">1000</span><span class="op">)</span>,</span>
<span>  max_bin <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">100</span>,<span class="fl">500</span>,<span class="fl">1000</span><span class="op">)</span>,</span>
<span>  cpus <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">3</span>,<span class="fl">7</span>,<span class="fl">11</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="fu">write_tsv</span><span class="op">(</span><span class="va">cpu_testing_grid</span>, <span class="st">"cpu_test_grid.tsv"</span><span class="op">)</span></span></code></pre></div>
<p>Include this line in the <code>sbatch</code> script above:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># replace $2 with the appropriate number of cmd line input, or the path to </span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co"># the lookup</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="bu">read</span> <span class="va">features</span> <span class="va">rounds</span> <span class="va">max_depth</span> <span class="va">max_bin</span> <span class="va">cpus</span> <span class="op">&lt;</span> <span class="op">&lt;(</span><span class="fu">sed</span> <span class="at">-n</span> <span class="va">${SLURM_ARRAY_TASK_ID}</span>p <span class="st">"</span><span class="va">$2</span><span class="st">"</span><span class="op">)</span></span></code></pre></div>
<div class="section level4">
<h4 id="submission-rate-results">Submission Rate Results<a class="anchor" aria-label="anchor" href="#submission-rate-results"></a>
</h4>
<div class="section level5">
<h5 id="cpu-10k-features-10k-rounds">CPU – 10k features, 10k rounds<a class="anchor" aria-label="anchor" href="#cpu-10k-features-10k-rounds"></a>
</h5>
<pre class="raw"><code>[chasem@login xgboost_testing]$ sacct -j 2437829_1000 --format=end
2023-02-14T06:34:59 

[chasem@login xgboost_testing]$ sacct -j 2437829_1 --format=submit
2023-02-14T06:23:05 

[chasem@login xgboost_testing]$ date -d 2023-02-14T06:34:59 +%s
1676378099

[chasem@login xgboost_testing]$ date -d 2023-02-14T06:23:05  +%s
1676377385</code></pre>
<p>This comes out to a submission rate of approximately 84 jobs/minute.
To run this on 20,000 genes with 10 fold CV where each fold is performed
separately without (stats theory based) data sharing speed ups, it would
take approximately 1.6 days.</p>
</div>
<div class="section level5">
<h5 id="cpu-81k-features-1k-rounds">CPU – 81k features, 1k rounds<a class="anchor" aria-label="anchor" href="#cpu-81k-features-1k-rounds"></a>
</h5>
<pre class="raw"><code>$ sacct -j 2409850_1000 --format=end
2023-02-12T21:33:06 

$ sacct -j 2409850_1 --format=submit
2023-02-12T20:59:55 

$ date -d 2023-02-12T21:33:06 +%s
1676259186

$ date -d 2023-02-12T20:59:55 +%s
1676257195</code></pre>
<p>This comes out to a submission rate of approximately 30 jobs/minute.
To run this on 20,000 genes with 10 fold cross validation where each
fold is performed separately, this would take approximately 4.5 days at
this rate.</p>
</div>
<div class="section level5">
<h5 id="gpu-a-typical-array-10k-features-10k-rounds-max_depth-2-max_bins-default-256">GPU – a typical array: 10k features, 10k rounds, max_depth 2,
max_bins default (256)<a class="anchor" aria-label="anchor" href="#gpu-a-typical-array-10k-features-10k-rounds-max_depth-2-max_bins-default-256"></a>
</h5>
<p>This is going to be hard to accept, but despite the fact that the GPU
execution is ~10 times as fast <em>per model</em>, since there are only
6 GPUs, the scheduling rate is slower by quite a lot (about 10x slower).
Note that I killed the array of 1000 after ~ 200 jobs had completed
because the pattern was obvious, and I realized I needed less jobs to
test the comparatively far smaller number of GPU</p>
<pre class="raw"><code>$ sacct -j 2448789_1 --format=submit
2023-02-14T21:32:21 

$ sacct -j 2448789_200 --format=end
2023-02-14T21:56:57 

$ date -d 2023-02-14T21:56:56 +%s
1676433416

$ date -d 2023-02-14T21:32:20 +%s
1676431940</code></pre>
<p>This comes out to a submission rate of 8.13 jobs / minute</p>
</div>
<div class="section level5">
<h5 id="gpu-chunked-array">GPU – chunked array<a class="anchor" aria-label="anchor" href="#gpu-chunked-array"></a>
</h5>
<p>It occurred to me that since each model executed so quickly, it might
be better to run more models per resource request, so instead of
submitting 1000 tasks separately, submit 100 tasks, each of which do 10
sequential tasks.</p>
<p>Model: 10k features, 10k rounds, max_depth 2, max_bin default
(256)</p>
<pre class="raw"><code>$ sacct -j 2442699_100 --format=end
2023-02-14T15:46:58 

$ sacct -j 2442699_1 --format=submit
2023-02-14T12:58:31

$ date -d 2023-02-14T15:46:58 +%s
1676411218

$ date -d 2023-02-14T12:58:31 +%s
1676401111</code></pre>
<p>this comes out (on a <em>per model basis</em>) to 5.9 jobs /
minute.</p>
</div>
</div>
</div>
</div>
<div class="section level2">
<h2 id="follow-ons">Follow-ons<a class="anchor" aria-label="anchor" href="#follow-ons"></a>
</h2>
<p>None of the serious ‘modelling backends’ are implemented in R or
python – packages in R and python provide interfaces to software
typically written in a compiled language. For the ‘modelling backends’
which have both R and python interfaces, the choice of R or python
<em>for that specific task</em> is arbitrary.</p>
<p>However, there are some features on which an informed choice can be
made. These are listed below.</p>
<ol style="list-style-type: decimal">
<li>Are there other tools available in one or the other platform which
ease handling the data at hand?</li>
<li>Does one of the classification and regression statistical learning
packages perform better at the data storage/preparation steps than
another? In this particular application, we have a large, sparse matrix
– possibly there are frameworks in one software, but not others to
handle this.</li>
<li>Are there built-in parallelization tools which provide an easier
path to scaling?</li>
</ol>
<div class="section level3">
<h3 id="specifically-for-xgboost">Specifically for XGBoost<a class="anchor" aria-label="anchor" href="#specifically-for-xgboost"></a>
</h3>
<p>There are (at least) two other implementations of the gradient
boosting tree method that are worth benchmarking on this data against
XGBoost:</p>
<ol style="list-style-type: decimal">
<li><p><a href="https://lightgbm.readthedocs.io/en/v3.3.2/" class="external-link">LightGBM</a>:
Developed and maintained at Microsoft. It is benchmarked against XGBoost
and is far faster</p></li>
<li><p><a href="https://catboost.ai/#benchmark" class="external-link">Catboost</a>: Even
faster than LightGLM, also claims higher performance than LightGLM and
XGBoost</p></li>
<li><p><a href="https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/gbm.html" class="external-link">H20/GBM</a>:
Looks like this software had a name change. According to the Catboost
benchmarks, H20 doesn’t perform as well. It has some serious developers
behind it, though. Worth checking out. H20 also includes other ML
algorithms.</p></li>
</ol>
</div>
<div class="section level3">
<h3 id="caret-tidymodels-and-scikit-learn">Caret, tidymodels and scikit-learn<a class="anchor" aria-label="anchor" href="#caret-tidymodels-and-scikit-learn"></a>
</h3>
<p>There are two well developed statistical learning softwares in R
which focus on the not-neural-nets: <a href="https://topepo.github.io/caret/index.html" class="external-link">caret</a> and the
comparatively newer <a href="https://www.tidymodels.org/" class="external-link">tidymodels</a>. Remember that no real
computation software is written in R or python – these are interfaces –
so the neural-net softwares are largely also available in R, though they
seem to be more popularly used in python.</p>
<p>In python, a comparable package to caret and tidymodels is <a href="https://scikit-learn.org/stable/index.html" class="external-link">scikit-learn</a>.</p>
<p>These softwares provide structures and functions which facilitate
doing the following common modelling tasks:</p>
<ol style="list-style-type: decimal">
<li>data preparation</li>
<li>test/train split</li>
<li>model selection and evaluation</li>
<li>hyperparameter tuning</li>
<li>cross validation</li>
<li>feature selection</li>
<li>… and more, like drop in parallelization on various back ends</li>
</ol>
<p>Each provides an interface to a large number of ‘modelling backends’,
ie xgboost, adaboost, catboost, fastglm, … . All of these are available
at the change of a switch and may be easily compared to one another both
in modelling performance and resource usage. These softwares also
provide built-in performance enhancers, eg minimizing work in n-fold CV
where it is possible by sharing data.</p>
<p>It would be foolish not to <em>rule these out</em> before starting a
coding task – the time commitment to learn one or all of them will pay
off in the ability to do far more with far less effort.</p>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Mateusiak Chase.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
